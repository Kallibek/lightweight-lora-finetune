{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "* PEFT technique: LoRA (Low-Rank Adaptation)\n",
    "* Model: GPT-2 fine-tuned for sequence classification (via AutoModelForSequenceClassification)\n",
    "* Evaluation approach: Hugging Face Trainer with the “accuracy” metric\n",
    "* Fine-tuning dataset: SST‑2 from the GLUE benchmark (binary sentiment classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c7e57b3fd54aa99d66eecb64095f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model evaluation: {'eval_loss': 3.7848360538482666, 'eval_accuracy': 0.5220000147819519, 'eval_runtime': 4.4033, 'eval_samples_per_second': 113.552, 'eval_steps_per_second': 14.308}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# preprocess function: tokenize and pad/truncate\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = (preds == labels).astype(np.float32).mean().item()\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "# load the SST-2 dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "# for quicker iteration, take subsets\n",
    "train_dataset = raw_datasets[\"train\"].shuffle(seed=42).select(range(10000))\n",
    "eval_dataset  = raw_datasets[\"validation\"].shuffle(seed=42).select(range(500))\n",
    "\n",
    "# load GPT-2 tokenizer & model for sequence classification\n",
    "model_name = \"gpt2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 has no pad token by default; use eos_token as pad\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# make sure the model config knows about that pad token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# tokenize datasets\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_eval  = eval_dataset.map(preprocess_function,  batched=True)\n",
    "\n",
    "# rename label column for Trainer compatibility\n",
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_eval  = tokenized_eval.rename_column(\"label\",  \"labels\")\n",
    "\n",
    "# set torch format\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_eval.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "# evaluate the original model\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/original_eval\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    logging_dir=\"/tmp/original_eval/logs\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "orig_metrics = trainer.evaluate()\n",
    "print(\"Original model evaluation:\", orig_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,984 || all params: 124,737,792 || trainable%: 0.23888830740245906\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# create a LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",      # sequence classification\n",
    "    inference_mode=False,     # training mode\n",
    "    r=8,                      # LoRA rank\n",
    "    lora_alpha=32,            # LoRA alpha\n",
    "    lora_dropout=0.1,         # dropout\n",
    ")\n",
    "\n",
    "# wrap the model in PEFT\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# print out the fraction of trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 13:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.633200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training arguments for PEFT fine-tuning\n",
    "peft_train_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/peft_training\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_dir=\"/tmp/peft_training/logs\",\n",
    ")\n",
    "\n",
    "# create a Trainer for fine-tuning\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_train_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,      # optional if you want eval during training\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# run training\n",
    "peft_trainer.train()\n",
    "\n",
    "# save only the adapter weights (LoRA)\n",
    "peft_model.save_pretrained(\"/tmp/peft_gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model evaluation: {'eval_loss': 0.5494799613952637, 'eval_accuracy': 0.7260000109672546, 'eval_runtime': 4.0868, 'eval_samples_per_second': 122.344, 'eval_steps_per_second': 15.415}\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# reload the base model architecture\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# make sure GPT‑2 knows its pad token\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# load the LoRA adapter into the base model\n",
    "peft_inference_model = PeftModel.from_pretrained(base_model, \"/tmp/peft_gpt2\")\n",
    "\n",
    "# evaluate the fine-tuned model\n",
    "inference_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/peft_eval\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_train=False,\n",
    "    logging_dir=\"/tmp/peft_eval/logs\",\n",
    ")\n",
    "inference_trainer = Trainer(\n",
    "    model=peft_inference_model,\n",
    "    args=inference_args,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "finetuned_metrics = inference_trainer.evaluate()\n",
    "print(\"Fine-tuned model evaluation:\", finetuned_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d3e0b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The foundation model initially achieved 52 % accuracy, which increased to 72 % after fine‑tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
